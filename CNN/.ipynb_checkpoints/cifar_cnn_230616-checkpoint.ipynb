{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7665cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24dbf3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 163s 1us/step\n"
     ]
    }
   ],
   "source": [
    "#Load and split dataset\n",
    "(train_images,train_labels), (test_images,test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "#Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images/255, test_images/255\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat',\n",
    "              'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c865e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[2])\n",
    "plt.xlabel(class_names[train_labels[2][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e6d53",
   "metadata": {},
   "source": [
    "CNN Architecture\n",
    "\n",
    "- We start by building the convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29480bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu', input_shape=(32,32,3)))\n",
    "model.add(layers.MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd7c8f",
   "metadata": {},
   "source": [
    "Layer 1\n",
    "- The input shape of our data will be 32,32,3 and we will process 32 filters of shape 3x3 over our input data\n",
    "- We will also apply the activation function relu to the output of each convolution operation (after the dot product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425ca46",
   "metadata": {},
   "source": [
    "Layer 2\n",
    "- This layer will perform maxpooling operation using 2x2 samples of stride 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330be727",
   "metadata": {},
   "source": [
    "Other layers\n",
    "- The next set of layers do very similar things but take as input the feature map from the previous layer. They also increase the frequency of filter from 32 to 64. Now our data shrinks in spatial dimensions as it is passed through the layers, meaning we can afford(computationaly) to add more depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a49fa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,320\n",
      "Trainable params: 56,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6e2e7",
   "metadata": {},
   "source": [
    "The summary shows that the depth our image increases but spatial dimensions decrease drastically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208826e",
   "metadata": {},
   "source": [
    "Adding Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fadb9",
   "metadata": {},
   "source": [
    "After completing the convo base, we need to take these extracted features and add a way to classify them. This is why we add the following layers to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bd10348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10)) #output layer corresponding to 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed387600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,570\n",
      "Trainable params: 122,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c0d8f",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e331989",
   "metadata": {},
   "source": [
    "Now, we will train and compile the model using recommended hyperparameters from tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0713429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 85s 53ms/step - loss: 1.5168 - accuracy: 0.4490 - val_loss: 1.2720 - val_accuracy: 0.5465\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 78s 50ms/step - loss: 1.1472 - accuracy: 0.5951 - val_loss: 1.1003 - val_accuracy: 0.6140\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 73s 47ms/step - loss: 0.9915 - accuracy: 0.6509 - val_loss: 1.0310 - val_accuracy: 0.6378\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.8960 - accuracy: 0.6865 - val_loss: 0.9584 - val_accuracy: 0.6697\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.8230 - accuracy: 0.7119 - val_loss: 0.9153 - val_accuracy: 0.6842\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 72s 46ms/step - loss: 0.7674 - accuracy: 0.7308 - val_loss: 0.9046 - val_accuracy: 0.6904\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 81s 52ms/step - loss: 0.7194 - accuracy: 0.7462 - val_loss: 0.8569 - val_accuracy: 0.7020\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 90s 58ms/step - loss: 0.6757 - accuracy: 0.7617 - val_loss: 0.8674 - val_accuracy: 0.7093\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 97s 62ms/step - loss: 0.6382 - accuracy: 0.7750 - val_loss: 0.8413 - val_accuracy: 0.7213\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 88s 57ms/step - loss: 0.5995 - accuracy: 0.7883 - val_loss: 0.8572 - val_accuracy: 0.7195\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "             loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs =10,\n",
    "                   validation_data=(test_images,test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd185f49",
   "metadata": {},
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "969185b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 3s - loss: 0.8572 - accuracy: 0.7195 - 3s/epoch - 10ms/step\n",
      "0.7195000052452087\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose = 2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854c407",
   "metadata": {},
   "source": [
    "This accuracy isn't bad for a simple model like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fcea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
