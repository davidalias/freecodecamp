{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ccbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe8eb8",
   "metadata": {},
   "source": [
    "For this example, we only need one piece of training data. We can also write our own poem or play and pass it to the \n",
    "network for training if we like. However, here we will be using Shakespeare's play 'Romeo and Juliet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f5c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras has this feature to save it as txt\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad1918d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "#Read, then decode\n",
    "text = open(path_to_file,'rb').read().decode(encoding='utf-8')\n",
    "#length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab83c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250]) #first 250 characters in the play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8bfe17",
   "metadata": {},
   "source": [
    "Encoding\n",
    "- We encode each unique character as a different integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de23833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4951c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab))\n",
    "print(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a21e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mapping from unique characters to indices\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86a38614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcb3285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "#function to convert numeric values to text\n",
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e171955",
   "metadata": {},
   "source": [
    "Creating Training Examples\n",
    "- Our task is to feed the model a sequence and have it return to us the next character\n",
    "- THis means, we need to split our text data from above into many shorter sequences that we can pass to the model as training example\n",
    "- We will use a sequence as input and another sequence as output, where this o/p sequence is the original sequence shifted one letter to the right\n",
    "- e.g., i/p: Hell and o/p: ello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84db17",
   "metadata": {},
   "source": [
    "First step is to create a stream of characters from our text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d1eb5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100 # length of sequence for training example\n",
    "examples_per_epoch = len(text)//(seq_length + 1) #for every training example, we use (seq_length + 1) characters as i/p and o/p combined\n",
    "\n",
    "#Creating training examples/targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c300193",
   "metadata": {},
   "source": [
    "Now, we use the batch method to turn this stream of characters into batches of desired length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0320f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder = True) \n",
    "#drop_reamainder will drop the remaining characters at the end, that can't be included in the batch of size 101 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94af4f",
   "metadata": {},
   "source": [
    "Use these sequences of length 101 and split them into input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3fd3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] #hell\n",
    "    target_text = chunk[1:] #ello\n",
    "    return input_text, target_text #hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target) # we use map to apply the above function to every entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27efb74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.MapDataset"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbc5f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "INPUT:  First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT:  irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "INPUT:  are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT:  re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\")\n",
    "    print('INPUT: ',int_to_text(x))\n",
    "    print('\\nOUTPUT: ',int_to_text(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105be6d",
   "metadata": {},
   "source": [
    "FInally, we need to make training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "817f1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #each epoch split into these many different batches\n",
    "VOCAB_SIZE = len(vocab) #number of unique characters\n",
    "EMBEDDING_DIM = 256 #Embedding dimension is how big we want every single vector to represent characters/words????\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory\n",
    "# it maintains a buffer in which it shuffle elements)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474aa96",
   "metadata": {},
   "source": [
    "Building the Model\n",
    "- We will use embedding, LSTM and a dense layer \n",
    "- Dense layer contains a node for each unique character in our training data. \n",
    "- The dense layer will give us a prob. distr. over all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfcf4585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None]),\n",
    "        #while making predictions, we donno how long each sequence is gonna be. So we give shape 'None', i.e., it is the length of sequence\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences = True, stateful = True, recurrent_initializer = 'glorot_uniform'),\n",
    "        # return_sequences = True, will give us output at every single time step, if set to False, only final output will be given\n",
    "        # glorot_uniform is a good default to pick for the values to start at in the LSTM\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d3558",
   "metadata": {},
   "source": [
    "Creating A Loss Function\n",
    "- Our Model will o/p (64,sequence_length,65) shaped tensor that represents the prob. distr. of each character, at each \n",
    "timestep, for every sequence in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68969c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) #(batch_size, sequence_length, vocab_length)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch) #ask our model for a prediction on our first batch of training data\n",
    "    print(example_batch_predictions.shape, '#(batch_size, sequence_length, vocab_length)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73d1624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[ 1.52606273e-03  4.39784210e-03  1.56661635e-03 ... -6.78597484e-04\n",
      "   -5.33037540e-03 -3.64726176e-04]\n",
      "  [ 2.29885569e-03  6.65342610e-04 -1.71128614e-03 ... -1.36823836e-03\n",
      "   -1.78331695e-03 -3.12160864e-03]\n",
      "  [-3.25368810e-03 -5.23829740e-03 -2.13816017e-03 ...  6.80872519e-03\n",
      "   -4.13131481e-03 -4.46233992e-03]\n",
      "  ...\n",
      "  [-1.77805428e-03 -4.60925838e-03 -1.15368022e-02 ...  4.41238331e-03\n",
      "    4.86281933e-03 -3.39989970e-03]\n",
      "  [ 7.27860257e-04 -6.06255955e-04 -8.13094806e-03 ...  2.84038065e-03\n",
      "    6.77505042e-03 -5.60484175e-03]\n",
      "  [-6.03105640e-04  2.19245767e-03 -5.47381351e-04 ...  8.92934622e-04\n",
      "    8.70068651e-03 -3.13523086e-03]]\n",
      "\n",
      " [[-2.75722472e-03  1.72068668e-03  7.96484761e-04 ...  2.91817565e-03\n",
      "    5.79182478e-03 -9.55520722e-04]\n",
      "  [-1.04677130e-03  4.50261915e-03  5.17377164e-04 ...  1.08733680e-03\n",
      "    6.44883793e-03 -3.06472182e-03]\n",
      "  [ 2.37814325e-04  7.31977355e-03  8.46851617e-05 ...  4.08830121e-04\n",
      "    7.04095466e-03 -4.47180681e-03]\n",
      "  ...\n",
      "  [-2.65300507e-04 -1.21756596e-03 -1.04601858e-02 ...  7.12127239e-03\n",
      "    4.22971789e-05 -5.33961039e-03]\n",
      "  [-1.32215384e-04 -3.23182903e-04 -1.22931926e-02 ...  5.30968467e-03\n",
      "   -5.27574914e-04 -2.63787596e-03]\n",
      "  [-5.04985545e-03 -6.99411146e-03 -1.09193912e-02 ...  1.06735453e-02\n",
      "   -1.78191892e-03 -4.03801119e-03]]\n",
      "\n",
      " [[-2.67510419e-03  3.01216426e-03 -3.39114806e-03 ...  5.12132840e-03\n",
      "    1.06586376e-04  5.08632045e-03]\n",
      "  [ 6.50847971e-04  4.60828887e-03 -4.94976481e-03 ...  2.56754272e-03\n",
      "    5.50874975e-03  4.53697983e-03]\n",
      "  [ 7.51493545e-03  7.58032640e-03 -7.17507815e-03 ...  2.64692795e-03\n",
      "   -4.03578579e-03  4.32500616e-03]\n",
      "  ...\n",
      "  [ 3.10701085e-03 -5.54619543e-03 -7.45490100e-03 ... -4.07659914e-04\n",
      "   -3.55446385e-03 -6.67864922e-03]\n",
      "  [-1.50905107e-03 -8.53277370e-03 -7.94898625e-03 ...  1.25874626e-03\n",
      "    1.19977235e-03 -7.23723415e-03]\n",
      "  [-1.90245046e-03 -8.41736607e-03 -6.19442761e-03 ...  4.01530974e-03\n",
      "    7.11405044e-03 -9.26073827e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3.76786105e-03  4.44444222e-03  1.53667061e-05 ... -6.91951253e-04\n",
      "   -1.07142108e-03 -9.27195302e-04]\n",
      "  [ 4.25773812e-03  4.27899882e-03  2.92670191e-03 ...  7.87478057e-04\n",
      "    1.35571510e-03  3.56664206e-03]\n",
      "  [-9.64985695e-04 -1.10330991e-04 -2.57822080e-03 ...  1.73735782e-03\n",
      "    3.64680076e-03  1.64750195e-03]\n",
      "  ...\n",
      "  [-3.47064203e-03 -3.20280413e-03 -4.94933268e-03 ...  9.07820370e-03\n",
      "   -2.49732425e-03 -1.39053992e-03]\n",
      "  [-3.98625154e-03 -5.69374720e-03 -4.07554302e-03 ...  8.02886672e-03\n",
      "    3.71660548e-03 -3.93389678e-03]\n",
      "  [-1.97936408e-03 -1.49435480e-03 -3.49359470e-03 ...  4.01387550e-03\n",
      "    3.97155853e-03 -4.85975016e-03]]\n",
      "\n",
      " [[-3.91165772e-03 -4.02394170e-03 -2.86703254e-03 ...  1.33004831e-03\n",
      "    3.30085796e-03 -1.84157130e-03]\n",
      "  [-6.83892891e-03 -6.91915816e-03 -5.23490645e-03 ...  2.54594488e-03\n",
      "    5.85373538e-03 -3.44381249e-03]\n",
      "  [-3.78726097e-03 -8.23081937e-03 -8.01845640e-03 ...  1.15871686e-03\n",
      "    6.77732099e-03 -6.14370452e-03]\n",
      "  ...\n",
      "  [-1.30608166e-03  4.71295207e-04 -8.11028015e-03 ...  3.11014801e-03\n",
      "   -6.55150227e-03 -2.03421107e-04]\n",
      "  [-5.40766446e-03 -4.52234223e-03 -1.02855265e-02 ...  3.66686098e-03\n",
      "   -1.44448178e-03 -3.69016011e-03]\n",
      "  [-8.86501279e-03 -1.02923559e-02 -1.04867667e-02 ...  9.50999931e-03\n",
      "   -3.31987208e-03 -6.25912659e-03]]\n",
      "\n",
      " [[ 1.42021826e-03  3.49143217e-03 -1.15967181e-04 ... -9.15960991e-04\n",
      "    2.24051625e-03 -2.54754373e-03]\n",
      "  [-1.54164550e-03  5.99532900e-03 -4.88498341e-03 ... -2.77417013e-04\n",
      "   -6.21204637e-03 -2.15986813e-03]\n",
      "  [ 4.53486992e-03  8.16858001e-03 -6.21981639e-03 ... -3.38324346e-04\n",
      "   -1.27608031e-02  7.01180426e-04]\n",
      "  ...\n",
      "  [-5.52740088e-03 -1.01908771e-02 -8.26104917e-03 ...  3.66469868e-03\n",
      "   -2.19367282e-03 -9.42428596e-03]\n",
      "  [-4.08151280e-03 -1.07835345e-02 -4.75986535e-03 ... -1.08186028e-03\n",
      "   -4.71822172e-03 -9.97574255e-03]\n",
      "  [-4.57281200e-03 -6.75057573e-03 -9.39284638e-03 ... -6.78987941e-04\n",
      "   -1.13078486e-02 -9.51962732e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#we can see that prediction is a collection of 64 (100,65) arrays, i.e., one for each element in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae2e1da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[ 0.00152606  0.00439784  0.00156662 ... -0.0006786  -0.00533038\n",
      "  -0.00036473]\n",
      " [ 0.00229886  0.00066534 -0.00171129 ... -0.00136824 -0.00178332\n",
      "  -0.00312161]\n",
      " [-0.00325369 -0.0052383  -0.00213816 ...  0.00680873 -0.00413131\n",
      "  -0.00446234]\n",
      " ...\n",
      " [-0.00177805 -0.00460926 -0.0115368  ...  0.00441238  0.00486282\n",
      "  -0.0033999 ]\n",
      " [ 0.00072786 -0.00060626 -0.00813095 ...  0.00284038  0.00677505\n",
      "  -0.00560484]\n",
      " [-0.00060311  0.00219246 -0.00054738 ...  0.00089293  0.00870069\n",
      "  -0.00313523]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#examine one prediction (one 2d array)\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b3460c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[ 1.5260627e-03  4.3978421e-03  1.5666164e-03 -3.3744862e-03\n",
      " -2.4987098e-03  3.0013584e-03 -7.8515004e-04 -5.5566966e-03\n",
      " -4.5616589e-03  1.5471394e-03 -4.5454642e-04 -2.0724447e-03\n",
      " -3.7062885e-03 -7.7394610e-03  9.5157546e-04 -6.8976851e-03\n",
      " -1.8667595e-03  7.4679416e-04 -3.0121536e-03  4.7531873e-03\n",
      "  7.7455101e-04  1.9817087e-03  5.2709780e-03 -3.2888567e-03\n",
      "  6.3460303e-04 -3.3614137e-03 -1.7593361e-03  3.8476447e-03\n",
      " -7.0484541e-04 -3.1365897e-03  1.6508361e-03  3.6797517e-03\n",
      " -1.1645490e-03  2.7079117e-03 -6.7240972e-04 -6.3039362e-03\n",
      " -3.3678252e-03 -4.5089750e-05 -1.8789032e-03 -2.3338529e-03\n",
      " -1.0582742e-03  4.0736998e-04  4.7858246e-03  1.2775387e-03\n",
      "  2.8697855e-03  1.9102667e-03 -8.1526570e-04 -2.1858558e-03\n",
      " -2.2530376e-03 -3.3462343e-03 -1.1918263e-03  2.2916633e-03\n",
      " -2.5648635e-03 -8.4485195e-04 -4.4924663e-03  2.8814538e-03\n",
      " -2.1200345e-03  1.2963250e-03 -2.4628232e-04  2.2931132e-03\n",
      "  4.8260868e-04  6.1183312e-04 -6.7859748e-04 -5.3303754e-03\n",
      " -3.6472618e-04], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#prediction at the first step\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "#its 65 values represent the probability of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70f971d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[18]\n",
      " [18]\n",
      " [31]\n",
      " [54]\n",
      " [32]\n",
      " [64]\n",
      " [15]\n",
      " [56]\n",
      " [39]\n",
      " [34]\n",
      " [14]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [44]\n",
      " [ 7]\n",
      " [52]\n",
      " [11]\n",
      " [60]\n",
      " [19]\n",
      " [24]\n",
      " [26]\n",
      " [20]\n",
      " [46]\n",
      " [ 6]\n",
      " [59]\n",
      " [ 7]\n",
      " [36]\n",
      " [35]\n",
      " [25]\n",
      " [16]\n",
      " [35]\n",
      " [41]\n",
      " [12]\n",
      " [47]\n",
      " [63]\n",
      " [35]\n",
      " [33]\n",
      " [41]\n",
      " [54]\n",
      " [27]\n",
      " [39]\n",
      " [17]\n",
      " [55]\n",
      " [56]\n",
      " [24]\n",
      " [ 2]\n",
      " [43]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [12]\n",
      " [ 7]\n",
      " [63]\n",
      " [ 3]\n",
      " [55]\n",
      " [17]\n",
      " [55]\n",
      " [55]\n",
      " [43]\n",
      " [38]\n",
      " [12]\n",
      " [48]\n",
      " [62]\n",
      " [38]\n",
      " [51]\n",
      " [49]\n",
      " [ 3]\n",
      " [36]\n",
      " [34]\n",
      " [ 4]\n",
      " [54]\n",
      " [53]\n",
      " [32]\n",
      " [ 2]\n",
      " [37]\n",
      " [55]\n",
      " [37]\n",
      " [ 0]\n",
      " [37]\n",
      " [14]\n",
      " [60]\n",
      " [ 3]\n",
      " [37]\n",
      " [14]\n",
      " [29]\n",
      " [39]\n",
      " [53]\n",
      " [46]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [39]\n",
      " [54]\n",
      " [64]\n",
      " [ 1]\n",
      " [25]\n",
      " [ 8]\n",
      " [34]\n",
      " [52]\n",
      " [ 3]\n",
      " [46]], shape=(100, 1), dtype=int64)\n",
      "[18 18 31 54 32 64 15 56 39 34 14  2  6 44  7 52 11 60 19 24 26 20 46  6\n",
      " 59  7 36 35 25 16 35 41 12 47 63 35 33 41 54 27 39 17 55 56 24  2 43  5\n",
      "  1 12  7 63  3 55 17 55 55 43 38 12 48 62 38 51 49  3 36 34  4 54 53 32\n",
      "  2 37 55 37  0 37 14 60  3 37 14 29 39 53 46  8  5  7 39 54 64  1 25  8\n",
      " 34 52  3 46]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"FFSpTzCraVB!,f-n;vGLNHh,u-XWMDWc?iyWUcpOaEqrL!e' ?-y$qEqqeZ?jxZmk$XV&poT!YqY\\nYBv$YBQaoh.'-apz M.Vn$h\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if we want to determine the predicted character, we need to sample the output distribution (pick a value based on prob. distr.)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples = 1) #draws 1 sample from a categorical distribution of 65 elements\n",
    "#here characters are picked not based on highest prob. but uses a prob. distr. to pick it\n",
    "print(sampled_indices)\n",
    "\n",
    "#now we can reshape that array and convert all integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1,-1))[0]\n",
    "print(sampled_indices)\n",
    "\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars #this is what the model predicted for training sequence 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d72b0",
   "metadata": {},
   "source": [
    "- Now we need to create a loss function that can compare that output to the expected output and give us some numeric value representing how close the two were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "512641d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fe8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
