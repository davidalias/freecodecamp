{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ccbb05",
   "metadata": {
    "id": "53ccbb05"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe8eb8",
   "metadata": {
    "id": "03fe8eb8"
   },
   "source": [
    "For this example, we only need one piece of training data. We can also write our own poem or play and pass it to the\n",
    "network for training if we like. However, here we will be using Shakespeare's play 'Romeo and Juliet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f5c2a6",
   "metadata": {
    "id": "84f5c2a6"
   },
   "outputs": [],
   "source": [
    "#keras has this feature to save it as txt\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad1918d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ad1918d",
    "outputId": "52d40075-f0ca-43ee-a524-ebbe7ca55ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "#Read, then decode\n",
    "text = open(path_to_file,'rb').read().decode(encoding='utf-8')\n",
    "#length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab83c21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ab83c21",
    "outputId": "aaed69f9-a908-4d5c-ddc2-9e86fb70ce8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250]) #first 250 characters in the play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8bfe17",
   "metadata": {
    "id": "2f8bfe17"
   },
   "source": [
    "Encoding\n",
    "- We encode each unique character as a different integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de23833d",
   "metadata": {
    "id": "de23833d"
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4951c6ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4951c6ee",
    "outputId": "413bc88c-028e-4a1e-e115-51880a259d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab))\n",
    "print(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a21e4ed",
   "metadata": {
    "id": "1a21e4ed"
   },
   "outputs": [],
   "source": [
    "#Create a mapping from unique characters to indices\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86a38614",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86a38614",
    "outputId": "90c2f675-6cee-4e5b-afb5-00e2c70509a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcb3285",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bcb3285",
    "outputId": "e32bf05c-03ce-416f-850c-8694c77ea71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "#function to convert numeric values to text\n",
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e171955",
   "metadata": {
    "id": "2e171955"
   },
   "source": [
    "Creating Training Examples\n",
    "- Our task is to feed the model a sequence and have it return to us the next character\n",
    "- THis means, we need to split our text data from above into many shorter sequences that we can pass to the model as training example\n",
    "- We will use a sequence as input and another sequence as output, where this o/p sequence is the original sequence shifted one letter to the right\n",
    "- e.g., i/p: Hell and o/p: ello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84db17",
   "metadata": {
    "id": "3b84db17"
   },
   "source": [
    "First step is to create a stream of characters from our text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d1eb5e2",
   "metadata": {
    "id": "5d1eb5e2"
   },
   "outputs": [],
   "source": [
    "seq_length = 100 # length of sequence for training example\n",
    "examples_per_epoch = len(text)//(seq_length + 1) #for every training example, we use (seq_length + 1) characters as i/p and o/p combined\n",
    "\n",
    "#Creating training examples/targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c300193",
   "metadata": {
    "id": "0c300193"
   },
   "source": [
    "Now, we use the batch method to turn this stream of characters into batches of desired length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0320f42a",
   "metadata": {
    "id": "0320f42a"
   },
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder = True)\n",
    "#drop_reamainder will drop the remaining characters at the end, that can't be included in the batch of size 101 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94af4f",
   "metadata": {
    "id": "8b94af4f"
   },
   "source": [
    "Use these sequences of length 101 and split them into input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3fd3fe8",
   "metadata": {
    "id": "b3fd3fe8"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] #hell\n",
    "    target_text = chunk[1:] #ello\n",
    "    return input_text, target_text #hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target) # we use map to apply the above function to every entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27efb74a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27efb74a",
    "outputId": "bd83c1a9-7fee-447f-b73b-25cedb2eaba0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.map_op._MapDataset"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbc5f72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbbc5f72",
    "outputId": "cbed7c67-b225-49b9-c3a0-7ec633403715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "INPUT:  First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT:  irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "INPUT:  are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT:  re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\")\n",
    "    print('INPUT: ',int_to_text(x))\n",
    "    print('\\nOUTPUT: ',int_to_text(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105be6d",
   "metadata": {
    "id": "6105be6d"
   },
   "source": [
    "FInally, we need to make training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "817f1978",
   "metadata": {
    "id": "817f1978"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #each epoch split into these many different batches\n",
    "VOCAB_SIZE = len(vocab) #number of unique characters\n",
    "EMBEDDING_DIM = 256 #Embedding dimension is how big we want every single vector to represent characters/words????\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory\n",
    "# it maintains a buffer in which it shuffle elements)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474aa96",
   "metadata": {
    "id": "4474aa96"
   },
   "source": [
    "Building the Model\n",
    "- We will use embedding, LSTM and a dense layer\n",
    "- Dense layer contains a node for each unique character in our training data.\n",
    "- The dense layer will give us a prob. distr. over all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfcf4585",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfcf4585",
    "outputId": "67d8af0d-86b2-4a71-e4b0-4de51839bbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None]),\n",
    "        #while making predictions, we donno how long each sequence is gonna be. So we give shape 'None', i.e., it is the length of sequence\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences = True, stateful = True, recurrent_initializer = 'glorot_uniform'),\n",
    "        # return_sequences = True, will give us output at every single time step, if set to False, only final output will be given\n",
    "        # glorot_uniform is a good default to pick for the values to start at in the LSTM\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d3558",
   "metadata": {
    "id": "b86d3558"
   },
   "source": [
    "Creating A Loss Function\n",
    "- Our Model will o/p (64,sequence_length,65) shaped tensor that represents the prob. distr. of each character, at each\n",
    "timestep, for every sequence in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68969c4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68969c4f",
    "outputId": "9d8cd5e2-e5a5-4f73-e7e8-9eaf1f5f7673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) #(batch_size, sequence_length, vocab_length)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch) #ask our model for a prediction on our first batch of training data\n",
    "    print(example_batch_predictions.shape, '#(batch_size, sequence_length, vocab_length)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73d1624f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73d1624f",
    "outputId": "da554969-3ea5-49e8-8cd2-141ac847fe3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[ 9.50125419e-03  1.19565462e-03 -1.13774498e-03 ... -2.88806623e-03\n",
      "    4.05510468e-03  1.68394786e-03]\n",
      "  [ 7.04359263e-03  6.35429146e-03 -1.21229037e-03 ... -3.40874493e-03\n",
      "    1.07180160e-02  5.95949823e-04]\n",
      "  [ 9.65206511e-03  1.21970885e-02  5.27461991e-03 ... -3.05358844e-04\n",
      "    9.05109383e-03  5.96746569e-03]\n",
      "  ...\n",
      "  [-7.02340435e-03  1.39329629e-03 -2.84144143e-03 ...  6.72004605e-03\n",
      "    2.62930267e-03  4.81669605e-03]\n",
      "  [-1.03976931e-02  6.58749836e-04 -1.00462250e-02 ...  6.62025157e-03\n",
      "   -1.18605653e-03  1.47616351e-03]\n",
      "  [-3.17057944e-03  8.26543663e-03 -3.56819015e-03 ...  5.23980195e-03\n",
      "    2.66896561e-04  7.35626929e-03]]\n",
      "\n",
      " [[-1.95488939e-03 -1.15381565e-03 -1.83486857e-03 ...  2.60391715e-03\n",
      "    2.72769085e-03 -6.66618580e-04]\n",
      "  [-2.65020411e-03 -3.17220110e-06 -1.75666250e-03 ... -2.02989695e-03\n",
      "    7.07071926e-03 -3.23510240e-03]\n",
      "  [-8.36339965e-03 -2.30313832e-04 -1.26838195e-03 ...  4.15644376e-03\n",
      "    4.79529332e-03 -2.00824626e-03]\n",
      "  ...\n",
      "  [-7.63098802e-03  3.74906557e-03 -1.01219108e-02 ...  1.64640415e-03\n",
      "    6.56446815e-03 -5.06771030e-03]\n",
      "  [-7.67216086e-03 -5.63393580e-04 -1.01635475e-02 ...  4.22930019e-03\n",
      "    1.17411073e-02 -9.16334055e-03]\n",
      "  [-5.19486610e-03  4.82105231e-03 -7.25802686e-03 ...  9.66610387e-05\n",
      "    1.29065532e-02 -3.32722883e-03]]\n",
      "\n",
      " [[-1.30079617e-03  3.84285743e-03  2.24184711e-03 ... -1.29725982e-03\n",
      "   -3.81855108e-03  4.32491396e-03]\n",
      "  [ 3.84005392e-03  1.03917848e-02  8.15048255e-03 ... -4.22788813e-04\n",
      "   -1.76798226e-03  9.85646062e-03]\n",
      "  [ 8.88423529e-03  4.62371251e-03  5.45093743e-03 ... -1.11297471e-03\n",
      "    3.86474817e-03  4.34244750e-03]\n",
      "  ...\n",
      "  [ 1.15737040e-02  5.09993732e-03  6.23148168e-03 ...  3.33116436e-03\n",
      "    1.37594109e-02  9.89919994e-03]\n",
      "  [ 2.23021279e-03  1.44986750e-03  6.50052400e-03 ...  2.29497161e-03\n",
      "    1.52919702e-02  7.54170585e-03]\n",
      "  [ 7.75659806e-04  1.36598083e-03  5.41201420e-03 ... -1.14585413e-03\n",
      "    1.79085843e-02  1.09016662e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-4.35207039e-05 -2.95295217e-03 -1.34781969e-03 ...  1.07167161e-03\n",
      "    2.10611091e-04  4.75007901e-03]\n",
      "  [ 4.65463102e-03  5.56890666e-03  3.37482220e-03 ...  1.64065370e-03\n",
      "    5.12172119e-04  1.09326849e-02]\n",
      "  [ 3.41404066e-03  1.78153883e-03 -1.83444307e-03 ...  8.71621817e-03\n",
      "    8.11946020e-03  7.87315425e-03]\n",
      "  ...\n",
      "  [ 4.34918748e-03  7.10481917e-03  7.65918894e-03 ...  4.52215970e-03\n",
      "    7.26400549e-03  9.56665725e-03]\n",
      "  [ 3.71348811e-03  6.69132266e-03  8.58650450e-03 ...  3.78042762e-03\n",
      "    4.35500359e-03  7.09781563e-03]\n",
      "  [ 1.14675169e-03  8.83618928e-03  8.73215590e-03 ...  1.68587593e-03\n",
      "    5.70500270e-05  8.46397784e-03]]\n",
      "\n",
      " [[-2.31836922e-03 -7.87371828e-04  6.54538721e-03 ... -3.95075139e-03\n",
      "    3.95276491e-03 -3.72168957e-03]\n",
      "  [ 1.18666119e-03  6.02670654e-04  7.62662943e-03 ...  5.40810754e-04\n",
      "   -1.23838917e-03 -3.55846481e-03]\n",
      "  [ 1.00700126e-04  1.97348045e-03  5.36110112e-03 ... -3.58759798e-03\n",
      "    4.02335450e-03 -6.69863727e-03]\n",
      "  ...\n",
      "  [ 4.71476931e-04 -1.96769717e-04  2.23989366e-04 ...  1.16751725e-02\n",
      "    6.88766222e-03  3.62950796e-03]\n",
      "  [ 6.84638461e-03  6.36107661e-03  4.99410834e-03 ...  8.15906748e-03\n",
      "    7.48022692e-03  9.62745864e-03]\n",
      "  [ 4.96861106e-03  4.47469065e-05  8.01067520e-03 ...  7.59801548e-03\n",
      "    9.33558866e-03  8.06156732e-03]]\n",
      "\n",
      " [[ 2.78080377e-04 -3.07024713e-03 -3.64162819e-03 ...  7.48121180e-03\n",
      "    7.54436664e-03  1.01338932e-03]\n",
      "  [ 1.72498554e-03 -6.23674551e-03 -3.28121986e-03 ...  5.25357155e-03\n",
      "    6.71971589e-03  2.56734155e-03]\n",
      "  [ 3.48567171e-03  4.91969869e-04 -1.02354912e-03 ... -1.49969943e-04\n",
      "    1.02532450e-02  7.00165518e-03]\n",
      "  ...\n",
      "  [ 7.25064520e-03  6.07773615e-03 -2.19985680e-03 ... -6.18230831e-03\n",
      "    1.31107457e-02 -5.54883294e-03]\n",
      "  [ 6.42271899e-03  8.78312252e-03 -2.49685720e-04 ... -7.69406511e-03\n",
      "    1.49026504e-02 -1.25272246e-03]\n",
      "  [ 7.77015928e-04  8.55369307e-03  2.32966896e-03 ... -5.73155098e-03\n",
      "    1.69118326e-02  7.21521489e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#we can see that prediction is a collection of 64 (100,65) arrays, i.e., one for each element in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae2e1da6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae2e1da6",
    "outputId": "cd8dd8ab-7341-4a71-f8d9-7d467df0e0fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[ 0.00950125  0.00119565 -0.00113774 ... -0.00288807  0.0040551\n",
      "   0.00168395]\n",
      " [ 0.00704359  0.00635429 -0.00121229 ... -0.00340874  0.01071802\n",
      "   0.00059595]\n",
      " [ 0.00965207  0.01219709  0.00527462 ... -0.00030536  0.00905109\n",
      "   0.00596747]\n",
      " ...\n",
      " [-0.0070234   0.0013933  -0.00284144 ...  0.00672005  0.0026293\n",
      "   0.0048167 ]\n",
      " [-0.01039769  0.00065875 -0.01004623 ...  0.00662025 -0.00118606\n",
      "   0.00147616]\n",
      " [-0.00317058  0.00826544 -0.00356819 ...  0.0052398   0.0002669\n",
      "   0.00735627]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#examine one prediction (one 2d array)\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b3460c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b3460c7",
    "outputId": "c95b8347-b18e-42cc-c804-bce5b8c9efe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[ 9.5012542e-03  1.1956546e-03 -1.1377450e-03  4.7183610e-03\n",
      "  5.5644722e-03  3.4959055e-03 -3.2487097e-03 -8.9778390e-04\n",
      "  1.0870774e-03 -2.2877681e-03 -2.0703176e-04  1.1756544e-03\n",
      " -3.7854617e-03 -1.4933019e-03  5.2501149e-03 -2.3365486e-05\n",
      "  1.7615437e-04 -3.8217758e-03 -5.4644244e-03  2.3330003e-04\n",
      " -1.6296285e-03 -9.1430591e-04  5.7554460e-04 -4.2708311e-04\n",
      " -2.6888244e-03  5.7822571e-04  1.0356912e-03 -3.4779133e-03\n",
      " -5.2930564e-03  4.5891386e-03  2.7047569e-04  9.3190465e-05\n",
      "  2.0134468e-03 -3.9827549e-03 -2.1932835e-03  1.4454343e-03\n",
      " -3.3242698e-04 -4.2910138e-03 -1.9990115e-03  2.8264048e-03\n",
      " -3.2877347e-03 -3.5806887e-03 -3.9789202e-03 -6.0600070e-03\n",
      "  1.4311330e-03  1.0862820e-03 -4.5173243e-03 -1.3049300e-03\n",
      "  2.4285729e-03  1.3694715e-03 -2.4794717e-03 -3.7528961e-03\n",
      " -6.8914741e-03 -1.4019492e-03  1.1776872e-03 -6.4224540e-04\n",
      "  7.7219796e-03 -1.9571960e-03  2.5359516e-03  7.3621408e-03\n",
      " -1.7208904e-03  7.7625655e-04 -2.8880662e-03  4.0551047e-03\n",
      "  1.6839479e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#prediction at the first step\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "#its 65 values represent the probability of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70f971d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "70f971d5",
    "outputId": "be40bfea-fcf8-4c08-fb4b-b237175b805e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[55]\n",
      " [48]\n",
      " [29]\n",
      " [34]\n",
      " [ 1]\n",
      " [63]\n",
      " [ 2]\n",
      " [12]\n",
      " [47]\n",
      " [16]\n",
      " [ 0]\n",
      " [26]\n",
      " [35]\n",
      " [14]\n",
      " [57]\n",
      " [ 7]\n",
      " [42]\n",
      " [51]\n",
      " [ 0]\n",
      " [16]\n",
      " [62]\n",
      " [ 3]\n",
      " [23]\n",
      " [39]\n",
      " [57]\n",
      " [ 2]\n",
      " [11]\n",
      " [14]\n",
      " [56]\n",
      " [25]\n",
      " [57]\n",
      " [55]\n",
      " [24]\n",
      " [29]\n",
      " [61]\n",
      " [ 2]\n",
      " [48]\n",
      " [44]\n",
      " [62]\n",
      " [18]\n",
      " [24]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [10]\n",
      " [28]\n",
      " [52]\n",
      " [37]\n",
      " [64]\n",
      " [50]\n",
      " [10]\n",
      " [24]\n",
      " [23]\n",
      " [40]\n",
      " [58]\n",
      " [22]\n",
      " [47]\n",
      " [ 6]\n",
      " [64]\n",
      " [25]\n",
      " [26]\n",
      " [43]\n",
      " [19]\n",
      " [31]\n",
      " [ 0]\n",
      " [ 8]\n",
      " [31]\n",
      " [40]\n",
      " [ 9]\n",
      " [57]\n",
      " [ 1]\n",
      " [17]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [18]\n",
      " [ 5]\n",
      " [45]\n",
      " [54]\n",
      " [57]\n",
      " [54]\n",
      " [51]\n",
      " [41]\n",
      " [31]\n",
      " [38]\n",
      " [16]\n",
      " [18]\n",
      " [ 3]\n",
      " [10]\n",
      " [16]\n",
      " [24]\n",
      " [64]\n",
      " [ 4]\n",
      " [15]\n",
      " [22]\n",
      " [21]\n",
      " [49]\n",
      " [42]\n",
      " [ 8]\n",
      " [57]], shape=(100, 1), dtype=int64)\n",
      "[55 48 29 34  1 63  2 12 47 16  0 26 35 14 57  7 42 51  0 16 62  3 23 39\n",
      " 57  2 11 14 56 25 57 55 24 29 61  2 48 44 62 18 24  3  4 10 28 52 37 64\n",
      " 50 10 24 23 40 58 22 47  6 64 25 26 43 19 31  0  8 31 40  9 57  1 17  6\n",
      "  1  8  3 18  5 45 54 57 54 51 41 31 38 16 18  3 10 16 24 64  4 15 22 21\n",
      " 49 42  8 57]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"qjQV y!?iD\\nNWBs-dm\\nDx$Kas!;BrMsqLQw!jfxFL$&:PnYzl:LKbtJi,zMNeGS\\n.Sb3s E, .$F'gpspmcSZDF$:DLz&CJIkd.s\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if we want to determine the predicted character, we need to sample the output distribution (pick a value based on prob. distr.)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples = 1) #draws 1 sample from a categorical distribution of 65 elements\n",
    "#here characters are picked not based on highest prob. but uses a prob. distr. to pick it\n",
    "print(sampled_indices)\n",
    "\n",
    "#now we can reshape that array and convert all integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1,-1))[0]\n",
    "print(sampled_indices)\n",
    "\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars #this is what the model predicted for training sequence 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d72b0",
   "metadata": {
    "id": "f06d72b0"
   },
   "source": [
    "- Now we need to create a loss function that can compare that output to the expected output and give us some numeric value representing how close the two were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "512641d4",
   "metadata": {
    "id": "512641d4"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d5f405",
   "metadata": {
    "id": "d5d5f405"
   },
   "source": [
    "Compiling The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "032185dd",
   "metadata": {
    "id": "032185dd"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30471854",
   "metadata": {
    "id": "30471854"
   },
   "source": [
    "- we are going to setup and configure our model to save checkpoint as it trains. This will allow us to load our model from a checkpoint and continue saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef125135",
   "metadata": {
    "id": "ef125135"
   },
   "outputs": [],
   "source": [
    "#Directory where checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "#Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}') #to save checkpoints for each epoch we run 'ckpt_{epoch}' is the prefix\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b085992",
   "metadata": {
    "id": "3b085992"
   },
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed73ba40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed73ba40",
    "outputId": "c6b4740e-f042-4031-fda6-7bed6258be73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "172/172 [==============================] - 23s 87ms/step - loss: 2.6057\n",
      "Epoch 2/40\n",
      "172/172 [==============================] - 15s 72ms/step - loss: 1.9130\n",
      "Epoch 3/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 1.6609\n",
      "Epoch 4/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 1.5224\n",
      "Epoch 5/40\n",
      "172/172 [==============================] - 14s 73ms/step - loss: 1.4371\n",
      "Epoch 6/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 1.3801\n",
      "Epoch 7/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 1.3336\n",
      "Epoch 8/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 1.2951\n",
      "Epoch 9/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 1.2601\n",
      "Epoch 10/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 1.2257\n",
      "Epoch 11/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 1.1915\n",
      "Epoch 12/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 1.1569\n",
      "Epoch 13/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 1.1206\n",
      "Epoch 14/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 1.0836\n",
      "Epoch 15/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 1.0446\n",
      "Epoch 16/40\n",
      "172/172 [==============================] - 15s 69ms/step - loss: 1.0045\n",
      "Epoch 17/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.9647\n",
      "Epoch 18/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.9230\n",
      "Epoch 19/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.8825\n",
      "Epoch 20/40\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.8446\n",
      "Epoch 21/40\n",
      "172/172 [==============================] - 15s 71ms/step - loss: 0.8064\n",
      "Epoch 22/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.7727\n",
      "Epoch 23/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.7383\n",
      "Epoch 24/40\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.7060\n",
      "Epoch 25/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.6782\n",
      "Epoch 26/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.6514\n",
      "Epoch 27/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.6298\n",
      "Epoch 28/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.6065\n",
      "Epoch 29/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.5873\n",
      "Epoch 30/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.5693\n",
      "Epoch 31/40\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.5556\n",
      "Epoch 32/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.5401\n",
      "Epoch 33/40\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.5283\n",
      "Epoch 34/40\n",
      "172/172 [==============================] - 15s 70ms/step - loss: 0.5155\n",
      "Epoch 35/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.5068\n",
      "Epoch 36/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.4955\n",
      "Epoch 37/40\n",
      "172/172 [==============================] - 14s 71ms/step - loss: 0.4859\n",
      "Epoch 38/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.4799\n",
      "Epoch 39/40\n",
      "172/172 [==============================] - 14s 70ms/step - loss: 0.4716\n",
      "Epoch 40/40\n",
      "172/172 [==============================] - 14s 69ms/step - loss: 0.4662\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=40, callbacks = checkpoint_callback)#epochs = 40, the more the epochs, the better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aRN4OK_yXOUW",
   "metadata": {
    "id": "aRN4OK_yXOUW"
   },
   "source": [
    "We'll rebuild the model from a checkpoint using a batch size of 1 so that we can feed one piece of text to the model and make it predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "jKK510cbXhHD",
   "metadata": {
    "id": "jKK510cbXhHD"
   },
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KdJCx0QOXwxy",
   "metadata": {
    "id": "KdJCx0QOXwxy"
   },
   "source": [
    "Once the model is finished training, we can find the latest checkpoint that stores the model weights using the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1CZERCEUX7D8",
   "metadata": {
    "id": "1CZERCEUX7D8"
   },
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1,None])) #[1,None] means input is length 1 and we don't know the 2nd dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aEc48-TbZ7l",
   "metadata": {
    "id": "6aEc48-TbZ7l"
   },
   "source": [
    "We can use the below function provided by tensorflow to generate some text using any starting string we like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "th-0DPs5bmxT",
   "metadata": {
    "id": "th-0DPs5bmxT"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  #Evaluation step (generating text using the learned model)\n",
    "\n",
    "  #Number of characters to generate\n",
    "  num_generate = 800\n",
    "\n",
    "  #Converting our start string to numbers(vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0) #e.g., [1 2 3] to [[1 2 3]] ??\n",
    "\n",
    "  #Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  #Low temperature results in more predictable text\n",
    "  #High temperature results in more surprising text\n",
    "  #Experiment to find the best setting\n",
    "  temperature = 1.0 #not needed necessarily\n",
    "\n",
    "  #Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    #remove batch dimension\n",
    "    predictions = tf.squeeze(predictions, 0) # [[]] to []\n",
    "\n",
    "    #using a categorical distribution to predict the character returned by the model\n",
    "\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1, 0].numpy()\n",
    "\n",
    "    #we pass the predicted character as the next input to the model along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "RDzuSBU2e4mD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDzuSBU2e4mD",
    "outputId": "8343d0e6-cce7-4f84-9cf0-6ea45907826c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a starting string: adventure\n",
      "adventure's death,\n",
      "He slily-but us all, that will deceive we free,\n",
      "We have a power in't igan; for whom I profess, I will\n",
      "pass'd the entious worm, you were in lawless horse!\n",
      "Thy of that store, whose children he was crows;\n",
      "All hour as this hath granted to the Tower,\n",
      "Only to time; for 'tis the city of a\n",
      "strong-prights and a brain:\n",
      "Ay, if he would please you, this must fold,\n",
      "Our word pooress,\n",
      "And most assisting on the lineary in his master.\n",
      "\n",
      "PETRUCHIO:\n",
      "Ay, this is some of that safety?\n",
      "\n",
      "First Murderer:\n",
      "Take that, and tell those grave me Duke of my master's fair.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "My gracious uncle, let me know my state,\n",
      "Ey with purgo set in her most unevent! They have ranced with no man shall\n",
      "good one Marcius is our case as to stuff a to\n",
      "come, thou didst usures it to my son's and blunt his lands and l\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "QsOZXKeogAgT",
   "metadata": {
    "id": "QsOZXKeogAgT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
