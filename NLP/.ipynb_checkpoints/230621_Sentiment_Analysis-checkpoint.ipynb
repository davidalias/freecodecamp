{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1856da5",
   "metadata": {},
   "source": [
    "IMDB movie review dataset from keras contains 25,000 reviews, where each one is already preprocessed and has a label of either positive or negative. Each review is encoded by integers that represents how common a word is in the entire dataset. For example, a word encoded by the integer 3 means that it is the 3rd most common word in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "145a6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import keras.preprocessing\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2abd611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 6s 0us/step\n",
      "17473536/17464789 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 88584 #all different words in the dataset\n",
    "\n",
    "MAXLEN =250\n",
    "BATCH_SIZE =64\n",
    "\n",
    "(train_data,train_labels), (test_data,test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2c0cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e99479d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6490d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697aa09",
   "metadata": {},
   "source": [
    "More Preprocessing\n",
    "- We notice here that some of the reviews are of different lengths. That's an issue bcoz we cannot pass different length data into our neural network. Therefore, we must make each review of the same length using following steps:\n",
    "     - if review >250 words, trim off the extra words\n",
    "     - if review < 250 words, add necessary amount of 0's  to make it equal to 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feae4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9103e3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1044b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     1,    14,    47,     8,    30,    31,     7,     4,\n",
       "         249,   108,     7,     4,  5974,    54,    61,   369,    13,\n",
       "          71,   149,    14,    22,   112,     4,  2401,   311,    12,\n",
       "          16,  3711,    33,    75,    43,  1829,   296,     4,    86,\n",
       "         320,    35,   534,    19,   263,  4821,  1301,     4,  1873,\n",
       "          33,    89,    78,    12,    66,    16,     4,   360,     7,\n",
       "           4,    58,   316,   334,    11,     4,  1716,    43,   645,\n",
       "         662,     8,   257,    85,  1200,    42,  1228,  2578,    83,\n",
       "          68,  3912,    15,    36,   165,  1539,   278,    36,    69,\n",
       "       44076,   780,     8,   106,    14,  6905,  1338,    18,     6,\n",
       "          22,    12,   215,    28,   610,    40,     6,    87,   326,\n",
       "          23,  2300,    21,    23,    22,    12,   272,    40,    57,\n",
       "          31,    11,     4,    22,    47,     6,  2307,    51,     9,\n",
       "         170,    23,   595,   116,   595,  1352,    13,   191,    79,\n",
       "         638,    89, 51428,    14,     9,     8,   106,   607,   624,\n",
       "          35,   534,     6,   227,     7,   129,   113], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15d9f7",
   "metadata": {},
   "source": [
    "Creating Model\n",
    "- Embeding layer, LSTM layer, dense node (to get our predicted sentiment)\n",
    "- 32 stands for vector dimension of output generated by embedding layer. We can change this value as we wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e6e8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32), # 32 dimension for every single word given as input to this layer\n",
    "    tf.keras.layers.Dense(1, activation ='sigmoid') #sigmoid squishes value b/w 0 and 1, hence can easily define positive or negative review\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2002cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          2834688   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ec2ad",
   "metadata": {},
   "source": [
    "Compile and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc88e002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 77s 119ms/step - loss: 0.3652 - acc: 0.8487 - val_loss: 0.3006 - val_acc: 0.8776\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 74s 119ms/step - loss: 0.2318 - acc: 0.9125 - val_loss: 0.2773 - val_acc: 0.8822\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 77s 124ms/step - loss: 0.1781 - acc: 0.9356 - val_loss: 0.3415 - val_acc: 0.8830\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 76s 121ms/step - loss: 0.1492 - acc: 0.9471 - val_loss: 0.2834 - val_acc: 0.8992\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 0.1267 - acc: 0.9552 - val_loss: 0.2979 - val_acc: 0.8898\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics =['acc'])\n",
    "history = model.fit(train_data, train_labels, epochs=5, validation_split = 0.2) #epochs =10, validation dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cca2af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 22s 29ms/step - loss: 0.3740 - acc: 0.8602\n",
      "[0.37397491931915283, 0.8601999878883362]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db5154",
   "metadata": {},
   "source": [
    "Making Predictions\n",
    "- Since our reviews are encoded, we need to convert any review that we write into that form so that the network can understand\n",
    "- To do that, we load the encodings from the dataset and use them to encode our own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4201863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]]\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    \n",
    "    return sequence.pad_sequences([tokens], MAXLEN)\n",
    "\n",
    "text = \"That movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e58cd748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6518825]\n",
      "[0.5604129]\n"
     ]
    }
   ],
   "source": [
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1,250))\n",
    "    pred = encoded_text\n",
    "    result = model.predict(pred)\n",
    "    print(result[0])\n",
    "    \n",
    "pos_review = 'That movie was so awesome. I really loved it and will watch it again because it was amazingly great'\n",
    "predict(pos_review)\n",
    "\n",
    "neg_review = \"that movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I have ever watched\"\n",
    "predict(neg_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa026fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
